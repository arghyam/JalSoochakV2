# Analytics ETL Service

A minimal, modular ETL service scaffold built with Spring Boot, PostgreSQL, and Kafka.

## ğŸ¯ Overview

This is a **starter/scaffold ETL service** designed to be easily expandable. It provides:

- Clean modular architecture
- PostgreSQL database integration
- Kafka consumer setup (placeholder)
- REST API endpoint for geo data
- Layered architecture following best practices

## ğŸ—ï¸ Architecture

```
com.jalsoochak.analyticsetlservice
 â”œâ”€â”€ config
 â”‚     â””â”€â”€ KafkaConfig              # Kafka consumer configuration
 â”‚
 â”œâ”€â”€ controller
 â”‚     â””â”€â”€ GeoController            # REST API endpoints
 â”‚
 â”œâ”€â”€ service
 â”‚     â””â”€â”€ GeoService               # Service interface
 â”‚     â””â”€â”€ impl
 â”‚           â””â”€â”€ GeoServiceImpl    # Service implementation
 â”‚
 â”œâ”€â”€ repository
 â”‚     â””â”€â”€ DimGeoRepository         # JPA repository
 â”‚
 â”œâ”€â”€ entity
 â”‚     â””â”€â”€ DimGeo                   # JPA entity
 â”‚
 â”œâ”€â”€ kafka
 â”‚     â””â”€â”€ GeoEventConsumer         # Kafka consumer (placeholder)
 â”‚
 â”œâ”€â”€ dto
 â”‚     â””â”€â”€ DimGeoResponseDTO        # Response DTO
 â”‚
 â”œâ”€â”€ mapper
 â”‚     â””â”€â”€ GeoMapper                # Entity to DTO mapper
 â”‚
 â””â”€â”€ EtlApplication                  # Main application class
```

## ğŸ› ï¸ Technology Stack

- **Java 21**
- **Spring Boot 3.2.0**
- **Spring Web** - REST API
- **Spring Data JPA** - Database access
- **Spring Kafka** - Kafka integration
- **PostgreSQL** - Database
- **Lombok** - Boilerplate reduction
- **Maven** - Build tool

## ğŸ“‹ Prerequisites

- Java 21+
- Maven 3.6+
- PostgreSQL 12+
- Kafka (for Kafka consumer functionality)

## ğŸš€ Getting Started

### 1. Database Setup

Create PostgreSQL database:

```sql
CREATE DATABASE jalsoochak_analytics_db;
```

The `dim_geo` table will be created automatically via JPA `ddl-auto=update`, or you can run the SQL script:

```sql
-- See src/main/resources/schema.sql
```

### 2. Configuration

Update `application.yml` with your database and Kafka settings:

```yaml
spring:
  datasource:
    url: jdbc:postgresql://localhost:5432/jalsoochak_analytics_db
    username: postgres
    password: postgres
  
  kafka:
    bootstrap-servers: localhost:9092
```

### 3. Run the Application

```bash
mvn spring-boot:run
```

The service will start on port `9090`.

## ğŸ“¡ API Endpoints

### GET /api/geos

Returns all geo records from `dim_geo` table.

**Response:**
```json
[
  {
    "geoId": "uuid",
    "tenantId": "uuid",
    "type": "STATE",
    "name": "Assam",
    "geoStateId": null,
    "geoDistrictId": null,
    "geoBlockId": null,
    "geoGpId": null,
    "geoVillageId": null,
    "updatedAt": "2024-01-01T00:00:00Z"
  }
]
```

## ğŸ”„ Kafka Integration

The service includes a placeholder Kafka consumer that subscribes to `geo.events` topic.

**Current behavior:** Only logs received messages.

**Future:** Will process and store geo events.

## ğŸ“ Project Structure

- **Entity Layer** (`entity/`) - JPA entities
- **Repository Layer** (`repository/`) - Data access
- **Service Layer** (`service/`) - Business logic
- **Controller Layer** (`controller/`) - REST endpoints
- **DTO Layer** (`dto/`) - Data transfer objects
- **Mapper Layer** (`mapper/`) - Entity/DTO conversion
- **Kafka Layer** (`kafka/`) - Kafka consumers
- **Config Layer** (`config/`) - Configuration classes

## ğŸ”§ Extending the Service

### Adding a New Table

1. Create entity in `entity/` package
2. Create repository in `repository/` package
3. Create DTO in `dto/` package
4. Create mapper in `mapper/` package
5. Create service interface and implementation
6. Create controller endpoint

### Adding a New Kafka Consumer

1. Create consumer class in `kafka/` package
2. Annotate with `@KafkaListener`
3. Configure topic in `application.yml` if needed

## ğŸ§ª Testing

Run tests:

```bash
mvn test
```

## ğŸ“ Notes

- This is a **scaffold/starter** project, not a full production implementation
- Database schema is managed via JPA `ddl-auto=update`
- Kafka consumer is a placeholder - implement processing logic as needed
- Follow the existing structure when adding new features

## ğŸ¯ Design Philosophy

- **Modularity** - Clear separation of concerns
- **Extensibility** - Easy to add new tables, APIs, and consumers
- **Simplicity** - Minimal complexity, maximum clarity
- **Best Practices** - Follows Spring Boot and Java best practices
